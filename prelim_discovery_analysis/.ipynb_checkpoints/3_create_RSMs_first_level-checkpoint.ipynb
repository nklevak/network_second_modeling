{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c950b6ca-2585-4a41-932a-6a90dd3ee40b",
   "metadata": {},
   "source": [
    "Create and save RSMs for all of the task/contrast maps per subject per encounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f782eefc-a6fb-4476-ad91-8f48ccd1b388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import nilearn\n",
    "import json\n",
    "import datetime\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import psutil\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "from matplotlib.patches import Patch\n",
    "from nilearn import plotting\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "from nilearn.glm.second_level import SecondLevelModel\n",
    "from nilearn.glm import threshold_stats_img\n",
    "from nilearn.image import concat_imgs, mean_img, index_img\n",
    "from nilearn.reporting import make_glm_report\n",
    "from nilearn import masking, image\n",
    "from nilearn import datasets\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fd8a93a-6f0a-4bb3-804e-c8b9ee1d928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the first level contrast maps are stored\n",
    "BASE_DIR = '/oak/stanford/groups/russpold/data/network_grant/discovery_BIDS_20250402/derivatives/'\n",
    "LEVEL = 'output_lev1_mni_no_ted_comp'\n",
    "\n",
    "# subjects in the discovery sample\n",
    "SUBJECTS = ['sub-s03', 'sub-s10', 'sub-s19', 'sub-s29', 'sub-s43']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d27ca11b-1924-4fdb-8da8-52621a1346a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant task and contrast and session data\n",
    "TASKS = [\"nBack\",\"flanker\",\"directedForgetting\",\"goNogo\", \"shapeMatching\", \"stopSignal\", \"cuedTS\", \"spatialTS\"]\n",
    "CONTRASTS = {}\n",
    "CONTRASTS[\"nBack\"] = [\"twoBack-oneBack\", \"match-mismatch\",\"task-baseline\",\"response_time\"] # the nback contrasts\n",
    "CONTRASTS[\"flanker\"] = [\"incongruent-congruent\", \"task-baseline\", \"incongruent-congruent\",\"response_time\"]\n",
    "CONTRASTS[\"directedForgetting\"] = [\"neg-con\", \"task-baseline\",\"response_time\"]\n",
    "CONTRASTS[\"goNogo\"] = [\"nogo_success-go\", \"nogo_success\",\"task-baseline\",\"response_time\"] # go_rtModel check\n",
    "CONTRASTS[\"shapeMatching\"] = [\"DDD\", \"DDS\", \"DNN\", \"DSD\", \"main_vars\", \"SDD\", \"SNN\", \"SSS\", \"task-baseline\",\"response_time\"]\n",
    "CONTRASTS[\"stopSignal\"] = [\"go\", \"stop_failure-go\", \"stop_failure\", \"stop_failure-stop_success\", \"stop_success-go\", \"stop_success\", \"stop_success-stop_failure\", \"task-baseline\",\"response_time\"]\n",
    "CONTRASTS[\"cuedTS\"] = [\"cue_switch_cost\", \"task_switch_cost\", \"task_switch_cue_switch-task_stay_cue_stay\", \"task-baseline\",\"response_time\"]\n",
    "CONTRASTS[\"spatialTS\"] = [\"cue_switch_cost\", \"task_switch_cost\", \"task_switch_cue_switch-task_stay_cue_stay\", \"task-baseline\",\"response_time\"]\n",
    "SESSIONS = ['ses-01', 'ses-02', 'ses-03', 'ses-04', 'ses-05', 'ses-06', 'ses-07', 'ses-08', 'ses-09','ses-10']\n",
    "\n",
    "# number of encounters each subject has with a task\n",
    "max_num_encounters = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bd5bed0-95b5-4e07-a94a-c99c847cc904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">[</span><span style=\"color: #000080; text-decoration-color: #000080\">get_dataset_dir</span><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">]</span> Dataset found in <span style=\"color: #800080; text-decoration-color: #800080\">/home/users/nklevak/nilearn_data/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">yeo_2011</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;34m[\u001b[0m\u001b[34mget_dataset_dir\u001b[0m\u001b[1;34m]\u001b[0m Dataset found in \u001b[35m/home/users/nklevak/nilearn_data/\u001b[0m\u001b[95myeo_2011\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed shape: (256, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "# Yeo network information\n",
    "# download Yeo atlas\n",
    "yeo = datasets.fetch_atlas_yeo_2011()\n",
    "yeo_7network = yeo['thick_7'] \n",
    "yeo_atlas_nifti = nib.load(yeo_7network)\n",
    "if yeo_atlas_nifti.get_fdata().ndim == 4:\n",
    "    yeo_data_3d = yeo_atlas_nifti.get_fdata().squeeze()\n",
    "    yeo_atlas_3d = nib.Nifti1Image(yeo_data_3d, yeo_atlas_nifti.affine, yeo_atlas_nifti.header)\n",
    "    print(f\"Fixed shape: {yeo_atlas_3d.shape}\")\n",
    "else:\n",
    "    yeo_atlas_3d = yeo_atlas_nifti\n",
    "\n",
    "network_names = {\n",
    "    1: \"Visual\",\n",
    "    2: \"Somatomotor\",\n",
    "    3: \"Dorsal Attention\",\n",
    "    4: \"Ventral Attention\",\n",
    "    5: \"Limbic\",\n",
    "    6: \"Frontoparietal Control\",\n",
    "    7: \"Default Mode\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33e97024-d809-4728-9943-aa4562bdcb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions:\n",
    "def build_contrast_map_path(base_dir, level, subject, session, task, contrast_name):\n",
    "    \"\"\"Build the file path for a contrast map.\"\"\"\n",
    "    filename = f'{subject}_{session}_task-{task}_contrast-{contrast_name}_rtmodel-rt_centered_stat-effect-size.nii.gz'\n",
    "    return os.path.join(base_dir, level, subject, task, 'indiv_contrasts', filename)\n",
    "\n",
    "def is_valid_contrast_map(img_path):\n",
    "    \"\"\"Check if a contrast map has sufficient variance and no NaN values.\"\"\"\n",
    "    try:\n",
    "        img = nib.load(img_path)\n",
    "        data = img.get_fdata()\n",
    "        return np.std(data) > 1e-10 and not np.isnan(data).any()\n",
    "    except Exception as e:\n",
    "        print(f\"Error validating {img_path}: {e}\")\n",
    "        return False\n",
    "        \n",
    "def clean_z_map_data(z_map, task, contrast_name, encounter):\n",
    "    \"\"\"Clean z-map data by handling NaN and infinity values.\"\"\"\n",
    "    data = z_map.get_fdata()\n",
    "    if np.isnan(data).any() or np.isinf(data).any():\n",
    "        data = np.nan_to_num(data)\n",
    "        z_map = nib.Nifti1Image(data, z_map.affine, z_map.header)\n",
    "        print(f\"Warning: Fixed NaN/Inf values in {task}:{contrast_name}:encounter-{encounter+1}\")\n",
    "    return z_map\n",
    "\n",
    "def save_rsm(rsm_results, filename):\n",
    "    \"\"\"\n",
    "    Simple save function\n",
    "    \n",
    "    Parameters:\n",
    "        rsm_results: RSM results dictionary\n",
    "        filename: filename to save (will add .pkl automatically)\n",
    "    \"\"\"\n",
    "    if not filename.endswith('.pkl'):\n",
    "        filename += '.pkl'\n",
    "    \n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(rsm_results, f)\n",
    "    \n",
    "    file_size = os.path.getsize(filename) / (1024 * 1024)\n",
    "    print(f\"Saved to {filename} ({file_size:.1f} MB)\")\n",
    "\n",
    "def load_rsm(filename):\n",
    "    \"\"\"\n",
    "    Simple load function\n",
    "    \n",
    "    Parameters:\n",
    "        filename: filename to load\n",
    "    \n",
    "    Returns:\n",
    "        rsm_results: Loaded RSM dictionary\n",
    "    \"\"\"\n",
    "    if not filename.endswith('.pkl'):\n",
    "        filename += '.pkl'\n",
    "    \n",
    "    with open(filename, 'rb') as f:\n",
    "        rsm_results = pickle.load(f)\n",
    "    \n",
    "    print(f\"Loaded from {filename}\")\n",
    "    return rsm_results\n",
    "\n",
    "def cleanup_memory():\n",
    "    \"\"\"\n",
    "    Clean up memory between batches\n",
    "    \"\"\"\n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    # Get memory info\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"Memory after cleanup: {memory.percent:.1f}% used ({memory.available/(1024**3):.1f}GB available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "197dc13f-e1cd-4b7a-9ad6-54bd4273d0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrange each subjects maps by which encounter num it is\n",
    "all_contrast_maps = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "encounter_maps = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))\n",
    "\n",
    "for task in TASKS:\n",
    "    for contrast_name in CONTRASTS[task]:\n",
    "        for subject in SUBJECTS:\n",
    "            overall_encounter_count = 0\n",
    "            \n",
    "            for session in SESSIONS:\n",
    "                contrast_map_path = build_contrast_map_path(BASE_DIR, LEVEL, subject, session, task, contrast_name)\n",
    "                \n",
    "                if os.path.exists(contrast_map_path):\n",
    "                    all_contrast_maps[task][contrast_name][subject].append(contrast_map_path)\n",
    "                    encounter_maps[task][contrast_name][subject][overall_encounter_count] = contrast_map_path\n",
    "                    overall_encounter_count += 1\n",
    "\n",
    "first_level_session_maps = all_contrast_maps\n",
    "first_level_encounter_maps = encounter_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e06cd0a-0311-4d78-88a7-e6a080fa20ad",
   "metadata": {},
   "source": [
    "## generate the RSMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2da2402-4761-4751-b4a1-ac82dc8374c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to gather maps of a certain task/contrast from first_level_encounter_maps\n",
    "def gather_tc_maps(req_tasks,req_contrasts,all_maps=first_level_encounter_maps,req_encounters=[0,1,2,3,4], req_subjects = SUBJECTS):\n",
    "    '''\n",
    "    Get a list of loaded niftis for specific task/contrast/encounter combinations of first level maps \n",
    "    \n",
    "    Parameters\n",
    "        req_tasks: list of tasks as strings (all tasks have to be from the TASKS dict)\n",
    "        req_contrasts: list of contrasts as strings (all tasks have to be from the CONTRASTS dict)\n",
    "        all_maps: [task][contrast_name][subject][overall_encounter_count] -> one map each (here it is in a filepath format)\n",
    "        req_encounters: list of encounter numbers that are requested (default is all 5)\n",
    "        req_subjects: list of subject id strings that are requested (default is all in SUBJECTS)\n",
    "    Return\n",
    "        specified_maps: list of loaded nifti files that fit the requested task, contrast, and encounter (this returns this for all subjects)\n",
    "        specified_descriptors: list of descriptions of each file (i.e. titles)\n",
    "        data_title: informative title for the RSM that will later be created\n",
    "    \n",
    "    '''\n",
    "    specified_maps = []\n",
    "    specified_descriptors = []\n",
    "    max_num_encounters = 5\n",
    "\n",
    "    if (len(req_tasks) == 0) or (len(req_contrasts) == 0):\n",
    "        return [], [], ''\n",
    "\n",
    "    for task in req_tasks:\n",
    "        if task not in TASKS:\n",
    "            print(f\"task {task} not in task masterlist\")\n",
    "            continue\n",
    "    \n",
    "        for contrast in req_contrasts:\n",
    "            if contrast not in CONTRASTS[task]: # make sure this contrast exists in the given task\n",
    "                print(f\"skipped for contrast {contrast} and task {task}\")\n",
    "                continue\n",
    "                \n",
    "            for subject in req_subjects:\n",
    "                if subject not in SUBJECTS:\n",
    "                    print(f\"subject: {subject} is not in this dataset, so skipped\")\n",
    "                    continue\n",
    "                    \n",
    "                for encounter in req_encounters:\n",
    "                    if encounter < 0 or encounter >= max_num_encounters:\n",
    "                        continue\n",
    "\n",
    "                    descriptor_name = f\"{subject}:encounter-0{encounter + 1}\"\n",
    "                            \n",
    "                    if task in all_maps.keys():\n",
    "                        if contrast in all_maps[task].keys():\n",
    "                            if subject in all_maps[task][contrast].keys():\n",
    "                                if encounter in all_maps[task][contrast][subject].keys():\n",
    "\n",
    "                                    map_data = all_maps[task][contrast][subject][encounter]\n",
    "                                    \n",
    "                                    # Check if file is already loaded\n",
    "                                    if isinstance(map_data, str):\n",
    "                                        # map_data is a file path, need to load it\n",
    "                                        try:\n",
    "                                            if os.path.exists(map_data):\n",
    "                                                loaded_map = nib.load(map_data)\n",
    "                                                specified_maps.append(loaded_map)\n",
    "                                                specified_descriptors.append(descriptor_name)\n",
    "                                            else:\n",
    "                                                print(f\"File not found: {map_data}\")\n",
    "                                                failed_loads.append((descriptor_name, \"File not found\"))\n",
    "                                        except Exception as e:\n",
    "                                            print(f\"Error loading {map_data}: {str(e)}\")\n",
    "                                    else:\n",
    "                                        print(f\"Unexpected data type for {descriptor_name}: {type(map_data)}\")\n",
    "                                        \n",
    "                                else:\n",
    "                                    print(f\"{task}|{contrast}|{subject}: {encounter}\")\n",
    "                                    continue\n",
    "                            else:\n",
    "                                print(f\"{task}|{contrast} subject {subject}\")\n",
    "                                continue\n",
    "                        else:\n",
    "                            print(f\"{task}:{contrast}\")\n",
    "                            continue\n",
    "                    else:\n",
    "                        print(f\"{task}\")\n",
    "                        continue\n",
    "    # create RSM title\n",
    "    data_title = ''\n",
    "    if (len(req_tasks) == 1):\n",
    "        data_title += f'Task:{req_tasks[0]}|'\n",
    "    else:  # more than 1 task\n",
    "        data_title += 'Task:'\n",
    "        for i, task in enumerate(req_tasks):\n",
    "            if (i != len(req_tasks) - 1):\n",
    "                data_title += f\"{task},\"\n",
    "            else:\n",
    "                data_title += f\"{task}\"\n",
    "        data_title += '|'\n",
    "\n",
    "    if (len(req_contrasts) == 1):\n",
    "        data_title += f'Contrast:{req_contrasts[0]}'\n",
    "    else:\n",
    "        data_title += 'Contrast:'\n",
    "        for i, contrast in enumerate(req_contrasts):\n",
    "            if (i != (len(req_contrasts) - 1)):\n",
    "                data_title += f\"{contrast},\"\n",
    "            else:\n",
    "                data_title += f\"{contrast}\"\n",
    "    \n",
    "    return specified_maps, specified_descriptors, data_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa07e6c4-6601-4d44-8123-316c8257452a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSING:\n",
      "goNogo|nogo_success-go|sub-s19: 4\n",
      "goNogo|nogo_success-go|sub-s29: 4\n",
      "goNogo|nogo_success-go|sub-s43: 4\n",
      "goNogo|nogo_success|sub-s19: 4\n",
      "goNogo|nogo_success|sub-s29: 4\n",
      "goNogo|nogo_success|sub-s43: 4\n",
      "goNogo|task-baseline|sub-s19: 4\n",
      "goNogo|task-baseline|sub-s29: 4\n",
      "goNogo|task-baseline|sub-s43: 4\n",
      "goNogo|response_time|sub-s19: 4\n",
      "goNogo|response_time|sub-s29: 4\n",
      "goNogo|response_time|sub-s43: 4\n",
      "cuedTS|cue_switch_cost|sub-s29: 4\n",
      "cuedTS|task_switch_cost|sub-s29: 4\n",
      "cuedTS|task_switch_cue_switch-task_stay_cue_stay|sub-s29: 4\n",
      "cuedTS|task-baseline|sub-s29: 4\n",
      "cuedTS|response_time|sub-s29: 4\n"
     ]
    }
   ],
   "source": [
    "task_contrast_all_maps = {}\n",
    "not_included = {}\n",
    "print(\"MISSING:\") # to see what maps are missing\n",
    "for task in TASKS:\n",
    "    task_contrast_all_maps[task] = {}\n",
    "    for contrast in CONTRASTS[task]:\n",
    "        task_contrast_all_maps[task][contrast] = {}\n",
    "        task_contrast_all_maps[task][contrast][\"maps_list\"] = []\n",
    "        task_contrast_all_maps[task][contrast][\"descriptors_list\"] = []\n",
    "        task_contrast_all_maps[task][contrast][\"data_title\"] = \"\"\n",
    "\n",
    "        req_tasks = [task]\n",
    "        req_contrasts = [contrast]\n",
    "\n",
    "        task_contrast_all_maps[task][contrast][\"maps_list\"],task_contrast_all_maps[task][contrast][\"descriptors_list\"],task_contrast_all_maps[task][contrast][\"data_title\"] = gather_tc_maps(req_tasks,req_contrasts,all_maps=first_level_encounter_maps,req_encounters=[0,1,2,3,4], req_subjects = SUBJECTS)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f36ec796-3546-41a4-8c55-5a2a049d2a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampling atlas:\n",
      "Using reference map from nBack, twoBack-oneBack\n",
      "Reference shape: (97, 115, 97)\n",
      "Atlas original shape: (256, 256, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/nklevak/miniconda3/lib/python3.9/site-packages/nilearn/image/resampling.py:805: FutureWarning: From release 0.13.0 onwards, this function will, by default, copy the header of the input image to the output. Currently, the header is reset to the default Nifti1Header. To suppress this warning and use the new behavior, set `copy_header=True`.\n",
      "  return resample_img(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlas resampled shape: (97, 115, 97)\n",
      "resampling complete!\n"
     ]
    }
   ],
   "source": [
    "# resample Yeo atlas to get each network activation for each task/contrast pair as well\n",
    "def resample_atlas(reference_map_object, yeo_atlas_data=yeo_atlas_3d):\n",
    "    \"\"\"\n",
    "    Resample atlas to match reference map\n",
    "    \n",
    "    Parameters:\n",
    "        reference_map_object: loaded NIfTI object to use as reference\n",
    "        yeo_atlas_data: loaded Yeo atlas\n",
    "    \n",
    "    Returns:\n",
    "        resampled_atlas: resampled atlas ready for use\n",
    "    \"\"\"\n",
    "    # Load reference if it's a path    \n",
    "    print(f\"Reference shape: {reference_map_object.shape}\")\n",
    "    print(f\"Atlas original shape: {yeo_atlas_data.shape}\")\n",
    "    \n",
    "    # Resample atlas\n",
    "    yeo_resampled = nilearn.image.resample_to_img(\n",
    "        yeo_atlas_data, \n",
    "        reference_map_object, \n",
    "        interpolation='nearest',\n",
    "        force_resample=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Atlas resampled shape: {yeo_resampled.shape}\")\n",
    "    return yeo_resampled\n",
    "\n",
    "# Get a reference map (use the first map from first task/contrast combination)\n",
    "def get_reference_map(task_contrast_all_maps=task_contrast_all_maps):\n",
    "    \"\"\"Get a reference map for resampling\"\"\"\n",
    "    for task in task_contrast_all_maps:\n",
    "        for contrast in task_contrast_all_maps[task]:\n",
    "            if len(task_contrast_all_maps[task][contrast][\"maps_list\"]) > 0:\n",
    "                reference_map = task_contrast_all_maps[task][contrast][\"maps_list\"][0]\n",
    "                print(f\"Using reference map from {task}, {contrast}\")\n",
    "                return reference_map\n",
    "    return None\n",
    "\n",
    "# resample the atlas\n",
    "print(\"Resampling atlas:\")\n",
    "reference_map = get_reference_map()\n",
    "if reference_map is None:\n",
    "    print(\"Error: No reference map found\")\n",
    "else:\n",
    "    yeo_atlas_resampled = resample_atlas(reference_map,yeo_atlas_3d)\n",
    "    print(\"resampling complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e53a0e2-9da8-4644-986f-22b9a01879f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern-based RSM computation for all networks and whole brain\n",
    "def compute_rsms(specified_maps, specified_descriptors, data_title, \n",
    "                              yeo_atlas_data=yeo_atlas_resampled, network_names=network_names, \n",
    "                              correlation_metric='pearson'):\n",
    "    '''\n",
    "    RSM computation using full voxel patterns for all networks and whole brain\n",
    "    \n",
    "    Parameters:\n",
    "        specified_maps: list of loaded nifti files\n",
    "        specified_descriptors: list of descriptions for each map \n",
    "        data_title: title for the RSM\n",
    "        yeo_atlas_data: resampled and loaded Yeo atlas (default yeo_atlas_resampled)\n",
    "        network_names: dictionary mapping network numbers to names\n",
    "        correlation_metric: 'pearson', 'spearman', or 'cosine'\n",
    "        include_whole_brain: whether to include whole brain RSM\n",
    "    '''\n",
    "    if len(specified_maps) == 0:\n",
    "        print(\"No maps provided for RSM computation\")\n",
    "        return {}, [], []\n",
    "    \n",
    "    # Get reference image for resampling\n",
    "    reference_img = specified_maps[0]\n",
    "    # print(f\"Reference image shape: {reference_img.shape}\")\n",
    "    \n",
    "    # Use pre-resampled atlas\n",
    "    if yeo_atlas_data is not None:\n",
    "        if hasattr(yeo_atlas_data, 'get_fdata'):\n",
    "            yeo_data = yeo_atlas_data.get_fdata()\n",
    "            print(f\"Using pre-resampled Yeo atlas shape: {yeo_data.shape}\")\n",
    "        else:\n",
    "            yeo_data = yeo_atlas_data\n",
    "            print(f\"Using Yeo atlas array with shape: {yeo_data.shape}\")\n",
    "    else:\n",
    "        print(\"Error: yeo_atlas_data is None\")\n",
    "        return {}, [], []\n",
    "    \n",
    "    # Get network labels and make the names\n",
    "    network_labels = np.unique(yeo_data[yeo_data > 0])\n",
    "    network_labels = [int(label) for label in network_labels]\n",
    "    print(f\"Found {len(network_labels)} networks: {network_labels}\")\n",
    "    network_labels_named = [f\"{label}_{network_names.get(label, f'Network{label}')}\" \n",
    "                               for label in network_labels]\n",
    "    print(f\"Network names: {network_labels_named}\")\n",
    "    \n",
    "    # Convert maps to data arrays and extract data for each network\n",
    "    n_maps = len(specified_maps)\n",
    "    \n",
    "    # Initialize storage for network data\n",
    "    network_data = {}\n",
    "    \n",
    "    # Add whole brain\n",
    "    network_data['whole_brain'] = []\n",
    "    \n",
    "    # Initialize network data storage\n",
    "    for i, network in enumerate(network_labels):\n",
    "        network_key = network_labels_named[i]\n",
    "        network_data[network_key] = []\n",
    "    \n",
    "    # Extract data from each map\n",
    "    for i, nifti_map in enumerate(specified_maps):\n",
    "        try:\n",
    "            # Get map data\n",
    "            if hasattr(nifti_map, 'get_fdata'):\n",
    "                map_data = nifti_map.get_fdata()\n",
    "            else:\n",
    "                map_data = nifti_map\n",
    "            \n",
    "            # Check shape alignment\n",
    "            if map_data.shape != yeo_data.shape:\n",
    "                print(f\"Warning: Map {i} shape {map_data.shape} doesn't match atlas shape {yeo_data.shape}\")\n",
    "                continue\n",
    "            \n",
    "            # Extract whole brain data\n",
    "            brain_mask = (yeo_data > 0) & (~np.isnan(map_data))\n",
    "            whole_brain_values = map_data[brain_mask]\n",
    "            network_data['whole_brain'].append(whole_brain_values)\n",
    "            \n",
    "            # Extract full activation patterns for each network\n",
    "            for j, network in enumerate(network_labels):\n",
    "                network_key = network_labels_named[j]\n",
    "                network_mask = (yeo_data == network) & (~np.isnan(map_data))\n",
    "                network_values = map_data[network_mask]\n",
    "                \n",
    "                if len(network_values) > 0:\n",
    "                    # Store FULL activation pattern for this network\n",
    "                    network_data[network_key].append(network_values)\n",
    "                else:\n",
    "                    # Store empty array for missing data\n",
    "                    network_data[network_key].append(np.array([]))\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing map {i} ({specified_descriptors[i]}): {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Compute RSMs\n",
    "    rsm_results = {}\n",
    "    \n",
    "    for region_name, region_data in network_data.items():\n",
    "        if len(region_data) != n_maps:\n",
    "            print(f\"Warning: {region_name} has {len(region_data)} values, expected {n_maps}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # correlate full activation patterns\n",
    "            rsm_matrix = np.zeros((n_maps, n_maps))\n",
    "            \n",
    "            for i in range(n_maps):\n",
    "                for j in range(n_maps):\n",
    "                    if i <= j:  # symmetric matrix\n",
    "                        \n",
    "                        # Handle empty arrays\n",
    "                        if len(region_data[i]) == 0 or len(region_data[j]) == 0:\n",
    "                            corr = 0.0\n",
    "                        else:\n",
    "                            if correlation_metric == 'pearson':\n",
    "                                corr, _ = pearsonr(region_data[i].flatten(), \n",
    "                                                 region_data[j].flatten())\n",
    "                            elif correlation_metric == 'spearman':\n",
    "                                corr, _ = spearmanr(region_data[i].flatten(), \n",
    "                                                  region_data[j].flatten())\n",
    "                            elif correlation_metric == 'cosine':\n",
    "                                dot_product = np.dot(region_data[i].flatten(), \n",
    "                                                   region_data[j].flatten())\n",
    "                                norm_i = np.linalg.norm(region_data[i].flatten())\n",
    "                                norm_j = np.linalg.norm(region_data[j].flatten())\n",
    "                                corr = dot_product / (norm_i * norm_j) if (norm_i * norm_j) > 0 else 0\n",
    "                        \n",
    "                        rsm_matrix[i, j] = corr\n",
    "                        rsm_matrix[j, i] = corr \n",
    "            \n",
    "            # Handle NaNs\n",
    "            rsm_matrix = np.nan_to_num(rsm_matrix, nan=0.0)\n",
    "            \n",
    "            # Calculate network size\n",
    "            if region_name == 'whole_brain':\n",
    "                n_voxels = len(region_data[0]) if len(region_data) > 0 and len(region_data[0]) > 0 else 0\n",
    "            else:\n",
    "                network_num = int(region_name.split('_')[0])\n",
    "                n_voxels = int(np.sum(yeo_data == network_num))\n",
    "            \n",
    "            rsm_results[region_name] = {\n",
    "                'rsm': rsm_matrix,\n",
    "                'n_voxels': n_voxels,\n",
    "                'analysis_type': 'pattern',\n",
    "                'network_name': region_name,\n",
    "                'full_title': data_title +'|'+ region_name,\n",
    "                'descriptors': specified_descriptors\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error computing RSM for {region_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Successfully computed pattern-based RSMs for {len(rsm_results)} regions\")\n",
    "    return rsm_results, network_labels_named, specified_descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fab39d1-d5cd-4dca-8638-446fc5aaab20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists!\n",
      "File exists!\n",
      "File exists!\n",
      "File exists!\n",
      "Loaded from complete_rsm_results.pkl\n"
     ]
    }
   ],
   "source": [
    "first_batch_tasks = [\"nBack\",\"flanker\",\"directedForgetting\"]\n",
    "second_batch_tasks = [\"goNogo\", \"shapeMatching\", \"stopSignal\"]\n",
    "third_batch_tasks = [ \"cuedTS\", \"spatialTS\"]\n",
    "\n",
    "# calculate batch 1 if necessary\n",
    "if os.path.exists(\"first_batch_rsm.pkl\") or os.path.exists(\"complete_rsm_results.pkl\"):\n",
    "    print(\"File exists!\")\n",
    "    \n",
    "else:\n",
    "    print(\"File not found...calculating now\")\n",
    "    rsm_results_all = {}\n",
    "    for task in task_contrast_all_maps:\n",
    "        rsm_results_all[task] = {}\n",
    "        \n",
    "        if task not in first_batch_tasks:\n",
    "            continue\n",
    "            \n",
    "        for contrast in task_contrast_all_maps[task]:\n",
    "            if len(task_contrast_all_maps[task][contrast][\"maps_list\"]) > 0:\n",
    "                rsm_results, network_labels, descriptors = compute_rsms(\n",
    "                    specified_maps=task_contrast_all_maps[task][contrast][\"maps_list\"],\n",
    "                    specified_descriptors=task_contrast_all_maps[task][contrast][\"descriptors_list\"],\n",
    "                    data_title=task_contrast_all_maps[task][contrast][\"data_title\"],\n",
    "                )\n",
    "                rsm_results_all[task][contrast] = rsm_results\n",
    "\n",
    "    # save first batch srm\n",
    "    save_rsm(rsm_results_all, \"first_batch_rsm\")\n",
    "    \n",
    "    del rsm_results_all  # Delete the large dictionary\n",
    "    cleanup_memory()\n",
    "\n",
    "# calculate batch 2 if necessary\n",
    "if os.path.exists(\"second_batch_rsm.pkl\") or os.path.exists(\"complete_rsm_results.pkl\"):\n",
    "    print(\"File exists!\")\n",
    "    \n",
    "else:\n",
    "    print(\"File not found...calculating now\")\n",
    "    rsm_results_all = {}\n",
    "    for task in task_contrast_all_maps:\n",
    "        rsm_results_all[task] = {}\n",
    "        \n",
    "        if task not in second_batch_tasks:\n",
    "            continue\n",
    "            \n",
    "        for contrast in task_contrast_all_maps[task]:\n",
    "            if len(task_contrast_all_maps[task][contrast][\"maps_list\"]) > 0:\n",
    "                rsm_results, network_labels, descriptors = compute_rsms(\n",
    "                    specified_maps=task_contrast_all_maps[task][contrast][\"maps_list\"],\n",
    "                    specified_descriptors=task_contrast_all_maps[task][contrast][\"descriptors_list\"],\n",
    "                    data_title=task_contrast_all_maps[task][contrast][\"data_title\"],\n",
    "                    yeo_atlas_data = yeo_atlas_resampled\n",
    "                )\n",
    "                rsm_results_all[task][contrast] = rsm_results\n",
    "\n",
    "    # save second batch srm\n",
    "    save_rsm(rsm_results_all, \"second_batch_rsm\")\n",
    "    \n",
    "    del rsm_results_all  # Delete the large dictionary\n",
    "    cleanup_memory()\n",
    "\n",
    "# calculate batch 3 if necessary\n",
    "if os.path.exists(\"third_batch_rsm.pkl\") or os.path.exists(\"complete_rsm_results.pkl\"):\n",
    "    print(\"File exists!\")\n",
    "    \n",
    "else:\n",
    "    print(\"File not found...calculating now\")\n",
    "    rsm_results_all = {}\n",
    "    for task in task_contrast_all_maps:\n",
    "        rsm_results_all[task] = {}\n",
    "        \n",
    "        if task not in third_batch_tasks:\n",
    "            continue\n",
    "            \n",
    "        for contrast in task_contrast_all_maps[task]:\n",
    "            if len(task_contrast_all_maps[task][contrast][\"maps_list\"]) > 0:\n",
    "                rsm_results, network_labels, descriptors = compute_rsms(\n",
    "                    specified_maps=task_contrast_all_maps[task][contrast][\"maps_list\"],\n",
    "                    specified_descriptors=task_contrast_all_maps[task][contrast][\"descriptors_list\"],\n",
    "                    data_title=task_contrast_all_maps[task][contrast][\"data_title\"],\n",
    "                    yeo_atlas_data = yeo_atlas_resampled\n",
    "                )\n",
    "                rsm_results_all[task][contrast] = rsm_results\n",
    "\n",
    "    # save third batch srm\n",
    "    save_rsm(rsm_results_all, \"third_batch_rsm\")\n",
    "    \n",
    "    del rsm_results_all  # Delete the large dictionary\n",
    "    cleanup_memory()\n",
    "\n",
    "# load all three RSM batches in one file called \"complete_rsm_results\"\n",
    "if os.path.exists(\"complete_rsm_results.pkl\"):\n",
    "    print(\"File exists!\")\n",
    "    all_rsms = load_rsm(\"complete_rsm_results\")\n",
    "else:\n",
    "    print(\"File not found...calculating now\")\n",
    "    \n",
    "    first_batch = load_rsm(\"first_batch_rsm\")\n",
    "    second_batch = load_rsm(\"second_batch_rsm\")\n",
    "    third_batch = load_rsm(\"third_batch_rsm\")\n",
    "    \n",
    "    # delete empty task keys from each batch dataset\n",
    "    for task in first_batch_tasks:\n",
    "        del second_batch[task]\n",
    "        del third_batch[task]\n",
    "    for task in second_batch_tasks:\n",
    "        del first_batch[task]\n",
    "        del third_batch[task]\n",
    "    for task in third_batch_tasks:\n",
    "        del first_batch[task]\n",
    "        del second_batch[task]\n",
    "\n",
    "    all_rsms = first_batch\n",
    "    all_rsms.update(second_batch)\n",
    "    all_rsms.update(third_batch)\n",
    "    save_rsm(all_rsms, \"complete_rsm_results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PDM Environment)",
   "language": "python",
   "name": "pdm-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c235f2-d393-4f5e-ad90-c67ea01026d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses a simple linear regression to get the trajectory of each parcel\n",
    "\n",
    "# 1. do it per task/contrast/individual and save (also save significance, intercept, etc)\n",
    "# 2. average the slopes per parcel in each task/contrast and save the group average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d42765b-f548-43b9-b583-bf44c56495fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import nilearn\n",
    "import json\n",
    "import datetime\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import psutil\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "from matplotlib.patches import Patch\n",
    "from nilearn import plotting\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "from nilearn.glm.second_level import SecondLevelModel\n",
    "from nilearn.glm import threshold_stats_img\n",
    "from nilearn.image import concat_imgs, mean_img, index_img\n",
    "from nilearn.reporting import make_glm_report\n",
    "from nilearn import masking, image\n",
    "from nilearn import datasets\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from nilearn.maskers import NiftiLabelsMasker\n",
    "from nilearn.plotting.find_cuts import find_cut_slices\n",
    "\n",
    "# Import shared utilities and configuration\n",
    "# need to do it this way because in a sub-directory (later turn config and utils into part of a package)\n",
    "from utils import (\n",
    "    TASKS, CONTRASTS, SUBJECTS, SESSIONS, ENCOUNTERS,\n",
    "    build_first_level_contrast_map_path, is_valid_contrast_map, clean_z_map_data,\n",
    "    convert_to_regular_dict, create_smor_atlas,load_smor_atlas, load_schaefer_atlas, cleanup_memory\n",
    ")\n",
    "from config import BASE_DIR, OUTPUT_DIRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70b0ca7e-da93-4065-883f-ad13a382bf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_req_contrasts = []\n",
    "for task in TASKS:\n",
    "    for contrast in CONTRASTS[task]:\n",
    "        if (contrast not in compiled_req_contrasts):\n",
    "            compiled_req_contrasts.append(contrast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a67ece76-9fac-488e-893d-2918c92e7dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Smorgasbord atlas...\n",
      "Atlas loaded with 429 regions\n",
      "Atlas shape: (193, 229, 193)\n"
     ]
    }
   ],
   "source": [
    "# smorgasbord stuff\n",
    "SMORG_PARCELLATED_DIR = OUTPUT_DIRS[\"smor\"]\n",
    "smor_files = {'mean':f'discovery_parcel_indiv_mean_updated'}\n",
    "smor_date_updated = '0111'\n",
    "indices = [1,2,3]\n",
    "# get smorgasbord atlas\n",
    "smorgasbord_atlas = load_smor_atlas()\n",
    "SMORG_IMG = smorgasbord_atlas.maps\n",
    "SMORG_DATA = SMORG_IMG.get_fdata()\n",
    "\n",
    "\n",
    "# atlas\n",
    "req_atlas = \"smor\"\n",
    "main_dir = SMORG_PARCELLATED_DIR\n",
    "main_files = smor_files\n",
    "date_updated = smor_date_updated\n",
    "atlas_obj = smorgasbord_atlas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd062eb3-0424-4768-afd8-ee46c7d957aa",
   "metadata": {},
   "source": [
    "# load the parcellated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d023e44-032e-48dc-a7cf-da880fd18885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: processed_data_dfs_2026/smor_parcel_dfs/discovery_parcel_indiv_mean_updated_0111_1_betas.pkl\n",
      "Loaded 2 subjects\n",
      "Loading: processed_data_dfs_2026/smor_parcel_dfs/discovery_parcel_indiv_mean_updated_0111_2_betas.pkl\n",
      "Loaded 2 subjects\n",
      "Loading: processed_data_dfs_2026/smor_parcel_dfs/discovery_parcel_indiv_mean_updated_0111_3_betas.pkl\n",
      "Loaded 1 subjects\n",
      "\n",
      "Total subjects loaded: 5\n",
      "Atlas: smor (429 regions)\n"
     ]
    }
   ],
   "source": [
    "file_type = \"default\" # can change to \"default\" to make this code use betas\n",
    "output_ending = \"_betas\"\n",
    "if (file_type == \"z\"):\n",
    "    output_ending = \"_z_scored\"\n",
    "        \n",
    "# Load mean parcel data from multiple files\n",
    "loaded_mean_parcel_dict = {}\n",
    "mean_filename = f\"{main_dir}/{main_files['mean']}_{date_updated}\"\n",
    "\n",
    "for num in indices:\n",
    "    fin_filename = f\"{mean_filename}_{num}{output_ending}.pkl\"\n",
    "    print(f\"Loading: {fin_filename}\")\n",
    "    \n",
    "    try:\n",
    "        with open(fin_filename, 'rb') as f:\n",
    "            dict_data = pickle.load(f)\n",
    "            loaded_mean_parcel_dict.update(dict_data)\n",
    "            print(f\"Loaded {len(dict_data)} subjects\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: File not found - {fin_filename}\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {fin_filename}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nTotal subjects loaded: {len(loaded_mean_parcel_dict)}\")\n",
    "print(f\"Atlas: {req_atlas} ({len(atlas_obj.labels)} regions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200c42e4-b41f-4a63-91fd-9f02a48ab1ed",
   "metadata": {},
   "source": [
    "# parcel trajectory modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1399892e-2037-47b3-a0bd-550eb0ea72b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ENCOUNTERS_EXPECTED = 5\n",
    "ENCOUNTER_CENTER = np.mean(np.arange(1, N_ENCOUNTERS_EXPECTED + 1)) # this will center at the same encounter for everybody\n",
    "\n",
    "def analyze_parcel_practice_effects(parcel_dict, subject, task, contrast, \n",
    "                                      encounters_str=ENCOUNTERS,\n",
    "                                      center_value=ENCOUNTER_CENTER):\n",
    "    \"\"\"\n",
    "    Analyze practice effects for individual parcels using betas as DVs.\n",
    "    \n",
    "    Model: beta_activation = B0 + B1 * encounter_centered\n",
    "    \n",
    "    IMPORTANT: Uses fixed centering based on expected N=5 encounters,\n",
    "               even when some encounters are missing.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    parcel_dict : dict\n",
    "        Format: [subject][task][contrast][encounter] -> DataFrame with 'activation' column (betas)\n",
    "    subject : str\n",
    "        Subject ID\n",
    "    task : str\n",
    "        Task name\n",
    "    contrast : str\n",
    "        Contrast name\n",
    "    encounters_str : list\n",
    "        List of encounter strings (default: ['01', '02', '03', '04', '05'])\n",
    "    center_value : float\n",
    "        Fixed centering value (default: 3.0 for 5 encounters)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    parcel_results : dict\n",
    "        Results for each parcel including effect sizes, stats, and diagnostics\n",
    "    \"\"\"\n",
    "    print(f\"{subject}/{task}/{contrast}\")\n",
    "    \n",
    "    # Get all individual parcels\n",
    "    first_encounter = parcel_dict[subject][task][contrast][encounters_str[0]]\n",
    "    all_parcels = first_encounter['region'].tolist()\n",
    "    parcel_results = {}\n",
    "    \n",
    "    for parcel in all_parcels:\n",
    "        # Extract trajectory for this specific parcel\n",
    "        trajectory = []\n",
    "        encounter_indices = []  # Track which encounters we have\n",
    "        \n",
    "        for i, enc in enumerate(encounters_str):\n",
    "            try: \n",
    "                df = parcel_dict[subject][task][contrast][enc]\n",
    "                activation = df[df['region'] == parcel]['activation'].iloc[0]\n",
    "    \n",
    "                try:\n",
    "                    activation = float(activation)\n",
    "                except (ValueError, TypeError):\n",
    "                    print(f\"Warning: Could not convert activation '{activation}' to float\")\n",
    "                    activation = np.nan\n",
    " \n",
    "                trajectory.append(activation)\n",
    "                encounter_indices.append(i + 1)  # 1-indexed (1, 2, 3, 4, 5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"for {subject}, encounter {enc} is missing for {task} {contrast}\")\n",
    "                break\n",
    "        \n",
    "        trajectory = np.array(trajectory, dtype=float)\n",
    "        encounter_indices = np.array(encounter_indices)\n",
    "        \n",
    "        # Remove NaN values\n",
    "        valid_mask = ~np.isnan(trajectory)\n",
    "        trajectory_clean = trajectory[valid_mask]\n",
    "        encounter_indices_clean = encounter_indices[valid_mask]\n",
    "        \n",
    "        if len(trajectory_clean) < 3:  # Need minimum 3 points for regression\n",
    "            print(f\"Insufficient data for parcel {parcel}\")\n",
    "            continue\n",
    "        \n",
    "        # CENTER using FIXED center value (same for all parcels/subjects)\n",
    "        # ensures B0 has consistent interpretation across analyses\n",
    "        encounters_centered = encounter_indices_clean - center_value\n",
    "        # ex (1,2,3,4,5): centered = [-2, -1, 0, 1, 2]\n",
    "        \n",
    "        n_encounters_actual = len(trajectory_clean)\n",
    "        \n",
    "        # Fit model: beta ~ encounter_centered\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
    "            encounters_centered, \n",
    "            trajectory_clean\n",
    "        )#inputs are x,y so trajectory_clean (beta activations) ~ the centered encounters\n",
    "        \n",
    "        # Calculate t-statistic for slope\n",
    "        t_stat_slope = slope / std_err if std_err > 0 else 0\n",
    "        \n",
    "        # Degrees of freedom\n",
    "        df = n_encounters_actual - 2\n",
    "        \n",
    "        # Calculate descriptive statistics\n",
    "        initial_activation = trajectory_clean[0]\n",
    "        final_activation = trajectory_clean[-1]\n",
    "        mean_activation = np.mean(trajectory_clean)\n",
    "        max_activation = np.max(trajectory_clean)\n",
    "        min_activation = np.min(trajectory_clean)\n",
    "        \n",
    "        # Effect sizes\n",
    "        absolute_change = final_activation - initial_activation\n",
    "        \n",
    "        if abs(initial_activation) > 0.001:\n",
    "            percent_change = (absolute_change / abs(initial_activation)) * 100\n",
    "        else:\n",
    "            percent_change = np.nan\n",
    "        \n",
    "        trajectory_std = np.std(trajectory_clean)\n",
    "        if trajectory_std > 0:\n",
    "            cohens_d = absolute_change / trajectory_std\n",
    "        else:\n",
    "            cohens_d = 0\n",
    "        \n",
    "        # Store results\n",
    "        parcel_results[parcel] = {\n",
    "            # Effect Sizes & Model Parameters\n",
    "            'beta_intercept': intercept,  # B0 - activation at centered encounter (enc 3)\n",
    "            'beta_slope': slope,  # B1 - practice effect (change per encounter)\n",
    "            'std_error': std_err,  # Standard error of slope\n",
    "            'ci_lower': slope - 1.96 * std_err,  # 95% CI lower bound\n",
    "            'ci_upper': slope + 1.96 * std_err,  # 95% CI upper bound\n",
    "            \n",
    "            # Statistical Tests\n",
    "            't_stat_slope': t_stat_slope,\n",
    "            'p_value': p_value,\n",
    "            'df': df,\n",
    "            'significant_change': (p_value < 0.05),\n",
    "            \n",
    "            # Model Fit & Diagnostics\n",
    "            'r_squared': r_value**2,\n",
    "            'n_encounters_actual': n_encounters_actual,\n",
    "            'encounters_included': encounter_indices_clean.tolist(),\n",
    "            'trajectory': trajectory_clean,\n",
    "            'centered_predictor': encounters_centered.tolist(),\n",
    "            \n",
    "            # Misc\n",
    "            'mean_activation': mean_activation,\n",
    "            'initial_activation': initial_activation,\n",
    "            'final_activation': final_activation,\n",
    "            'max_activation': max_activation,\n",
    "            'min_activation': min_activation,\n",
    "            'activation_range': max_activation - min_activation,\n",
    "            'absolute_change': absolute_change,\n",
    "            'percent_change': percent_change,\n",
    "            'cohens_d': cohens_d,\n",
    "        }\n",
    "    \n",
    "    return parcel_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd61282-af66-447e-8335-24175f99a141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the parcel trajectory results per subject\n",
    "parcel_traj_results = {}\n",
    "for subj in SUBJECTS:\n",
    "    parcel_traj_results[subj] = {}\n",
    "\n",
    "    for task in TASKS:\n",
    "        parcel_traj_results[subj][task] = {}\n",
    "\n",
    "        for contrast in CONTRASTS[task]:\n",
    "            try:\n",
    "                parcel_traj_results[subj][task][contrast] = analyze_parcel_practice_effects(\n",
    "                    loaded_mean_parcel_dict, subj, task, contrast\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {subj}/{task}/{contrast}: {e}\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30a7ad65-6cc9-411a-88f4-e7007222e6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for sub-s03 there are 40 specific task/contrast combos loaded\n",
      "for sub-s10 there are 40 specific task/contrast combos loaded\n",
      "for sub-s19 there are 40 specific task/contrast combos loaded\n",
      "for sub-s29 there are 40 specific task/contrast combos loaded\n",
      "for sub-s43 there are 40 specific task/contrast combos loaded\n"
     ]
    }
   ],
   "source": [
    "# verify numbers for each\n",
    "for subj in SUBJECTS:\n",
    "    count = 0\n",
    "    \n",
    "    for task in parcel_traj_results[subj].keys():\n",
    "        for contrast in parcel_traj_results[subj][task].keys():\n",
    "            count += 1\n",
    "    print(f\"for {subj} there are {count} specific task/contrast combos loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1180a146-84e4-4ba1-b463-0e49dd2d87ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the individual files\n",
    "with open(f'{mean_filename}{output_ending}_indiv_slopes.pkl', 'wb') as f:\n",
    "    pickle.dump(parcel_traj_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fcdabe-3536-48ef-ad7d-af78cad95bcf",
   "metadata": {},
   "source": [
    "# step 2: per task/contrast average the slopes across all subjects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d30e26a3-cf4c-4128-be5e-444471f06f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing nBack/twoBack-oneBack:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 14 (3.3%)\n",
      "Processing nBack/match-mismatch:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 23 (5.4%)\n",
      "Processing nBack/task-baseline:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 29 (6.8%)\n",
      "Processing nBack/response_time:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 5 (1.2%)\n",
      "Processing flanker/incongruent-congruent:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 10 (2.3%)\n",
      "Processing flanker/task-baseline:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 14 (3.3%)\n",
      "Processing directedForgetting/neg-con:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 21 (4.9%)\n",
      "Processing directedForgetting/task-baseline:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 23 (5.4%)\n",
      "Processing directedForgetting/response_time:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 43 (10.0%)\n",
      "Processing goNogo/nogo_success-go:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 8 (1.9%)\n",
      "Processing goNogo/nogo_success:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 9 (2.1%)\n",
      "Processing goNogo/task-baseline:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 31 (7.2%)\n",
      "Processing goNogo/response_time:\n",
      "Error processing sub-s03/goNogo/response_time: 'response_time'\n",
      "Error processing sub-s10/goNogo/response_time: 'response_time'\n",
      "Error processing sub-s19/goNogo/response_time: 'response_time'\n",
      "Error processing sub-s29/goNogo/response_time: 'response_time'\n",
      "Error processing sub-s43/goNogo/response_time: 'response_time'\n",
      "No parcels found (no subjects have data for this contrast): goNogo|response_time\n",
      "Processing shapeMatching/DDD:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 15 (3.5%)\n",
      "Processing shapeMatching/DDS:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 13 (3.0%)\n",
      "Processing shapeMatching/DNN:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 8 (1.9%)\n",
      "Processing shapeMatching/DSD:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 23 (5.4%)\n",
      "Processing shapeMatching/main_vars:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 7 (1.6%)\n",
      "Processing shapeMatching/SDD:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 12 (2.8%)\n",
      "Processing shapeMatching/SNN:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 14 (3.3%)\n",
      "Processing shapeMatching/SSS:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 13 (3.0%)\n",
      "Processing shapeMatching/task-baseline:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 14 (3.3%)\n",
      "Processing shapeMatching/response_time:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 28 (6.5%)\n",
      "Processing stopSignal/go:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 48 (11.2%)\n",
      "Processing stopSignal/stop_failure-go:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 23 (5.4%)\n",
      "Processing stopSignal/stop_failure:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 55 (12.8%)\n",
      "Processing stopSignal/stop_failure-stop_success:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 10 (2.3%)\n",
      "Processing stopSignal/stop_success-go:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 8 (1.9%)\n",
      "Processing stopSignal/stop_success:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 19 (4.4%)\n",
      "Processing stopSignal/stop_success-stop_failure:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 10 (2.3%)\n",
      "Processing stopSignal/task-baseline:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 51 (11.9%)\n",
      "Processing stopSignal/response_time:\n",
      "Error processing sub-s03/stopSignal/response_time: 'response_time'\n",
      "Error processing sub-s10/stopSignal/response_time: 'response_time'\n",
      "Error processing sub-s19/stopSignal/response_time: 'response_time'\n",
      "Error processing sub-s29/stopSignal/response_time: 'response_time'\n",
      "Error processing sub-s43/stopSignal/response_time: 'response_time'\n",
      "No parcels found (no subjects have data for this contrast): stopSignal|response_time\n",
      "Processing cuedTS/cue_switch_cost:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 5 (1.2%)\n",
      "Processing cuedTS/task_switch_cost:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 20 (4.7%)\n",
      "Processing cuedTS/task_switch_cue_switch-task_stay_cue_stay:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 21 (4.9%)\n",
      "Processing cuedTS/task-baseline:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 5 (1.2%)\n",
      "Processing cuedTS/response_time:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 8 (1.9%)\n",
      "Processing spatialTS/cue_switch_cost:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 39 (9.1%)\n",
      "Processing spatialTS/task_switch_cost:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 2 (0.5%)\n",
      "Processing spatialTS/task_switch_cue_switch-task_stay_cue_stay:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 9 (2.1%)\n",
      "Processing spatialTS/task-baseline:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 31 (7.2%)\n",
      "Processing spatialTS/response_time:\n",
      "Completed: 429 parcels\n",
      "Group-level significant (p<0.05): 21 (4.9%)\n",
      "\n",
      "Averaging complete for 40 task/contrasts!\n"
     ]
    }
   ],
   "source": [
    "# create an averaged parcel df across all participants and save it to a file\n",
    "avg_parcel_traj_results = {}\n",
    "\n",
    "count_success = 0\n",
    "for task in TASKS:\n",
    "    avg_parcel_traj_results[task] = {}\n",
    "\n",
    "    for contrast in CONTRASTS[task]:\n",
    "        print(f\"Processing {task}/{contrast}:\")\n",
    "        avg_parcel_traj_results[task][contrast] = {}\n",
    "\n",
    "        # Collect all parcel data across subjects\n",
    "        parcel_data = defaultdict(list)\n",
    "        \n",
    "        for subj in SUBJECTS:\n",
    "            try:\n",
    "                curr_res = parcel_traj_results[subj][task][contrast]\n",
    "                \n",
    "                # For each parcel in this subject's results\n",
    "                for parcel_name, parcel_stats in curr_res.items():\n",
    "                    parcel_data[parcel_name].append(parcel_stats)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {subj}/{task}/{contrast}: {e}\")\n",
    "                continue\n",
    "\n",
    "        \n",
    "        # Calculate averages for each parcel\n",
    "        for parcel_name, subject_data_list in parcel_data.items():\n",
    "            if len(subject_data_list) == 0:\n",
    "                continue\n",
    "            \n",
    "            n_subjects = len(subject_data_list)  # ADD THIS\n",
    "\n",
    "            # INDIVIDUAL\n",
    "            slopes = np.array([data['beta_slope'] for data in subject_data_list])\n",
    "            intercepts = np.array([data['beta_intercept'] for data in subject_data_list])\n",
    "            r_squareds = np.array([data['r_squared'] for data in subject_data_list])\n",
    "            std_errors = np.array([data['std_error'] for data in subject_data_list])\n",
    "            individual_p_values = np.array([data['p_value'] for data in subject_data_list])\n",
    "            initial_activations = np.array([data['initial_activation'] for data in subject_data_list])\n",
    "            final_activations = np.array([data['final_activation'] for data in subject_data_list])\n",
    "            \n",
    "            percent_changes = np.array([data['percent_change'] for data in subject_data_list \n",
    "                                       if not np.isnan(data['percent_change'])])\n",
    "            cohens_ds = np.array([data['cohens_d'] for data in subject_data_list])\n",
    "            \n",
    "            max_activations = np.array([data['max_activation'] for data in subject_data_list])\n",
    "            min_activations = np.array([data['min_activation'] for data in subject_data_list])\n",
    "            activation_ranges = np.array([data['activation_range'] for data in subject_data_list])\n",
    "            trajectories = [data['trajectory'] for data in subject_data_list]\n",
    "\n",
    "            # GROUP-LEVEL\n",
    "            slope_mean = np.mean(slopes)\n",
    "            slope_std = np.std(slopes, ddof=1)\n",
    "            slope_sem = slope_std / np.sqrt(n_subjects)\n",
    "            \n",
    "            # One-sample t-test\n",
    "            if n_subjects > 1:\n",
    "                group_t_stat, group_p_value = stats.ttest_1samp(slopes, 0)\n",
    "                \n",
    "                # ADD: 95% Confidence interval\n",
    "                t_crit = stats.t.ppf(0.975, df=n_subjects-1)\n",
    "                slope_ci_lower = slope_mean - t_crit * slope_sem\n",
    "                slope_ci_upper = slope_mean + t_crit * slope_sem\n",
    "                \n",
    "                # ADD: Cohen's d for group effect\n",
    "                group_cohens_d = slope_mean / slope_std if slope_std > 0 else 0\n",
    "            else:\n",
    "                group_t_stat = np.nan\n",
    "                group_p_value = np.nan\n",
    "                slope_ci_lower = np.nan\n",
    "                slope_ci_upper = np.nan\n",
    "                group_cohens_d = 0\n",
    "\n",
    "            # calculate trajectory vals since diff contrasts/subj have different numbers of encounters\n",
    "            if len(trajectories) > 0:\n",
    "                # Check if all trajectories are the same length\n",
    "                trajectory_lengths = [len(traj) for traj in trajectories]\n",
    "                \n",
    "                if len(set(trajectory_lengths)) == 1:\n",
    "                    # All same length\n",
    "                    trajectory_array = np.array(trajectories)\n",
    "                    trajectory_mean = np.mean(trajectory_array, axis=0)\n",
    "                    trajectory_std = np.std(trajectory_array, axis=0, ddof=1)\n",
    "                    trajectory_sem = trajectory_std / np.sqrt(len(trajectories))\n",
    "                    trajectory_n = np.full(len(trajectory_mean), len(trajectories))\n",
    "                else:\n",
    "                    # Different lengths - use padding\n",
    "                    max_length = max(trajectory_lengths)\n",
    "                    padded_trajectories = []\n",
    "                    \n",
    "                    for traj in trajectories:\n",
    "                        if len(traj) < max_length:\n",
    "                            padded = np.full(max_length, np.nan)\n",
    "                            padded[:len(traj)] = traj\n",
    "                            padded_trajectories.append(padded)\n",
    "                        else:\n",
    "                            padded_trajectories.append(traj)\n",
    "                    \n",
    "                    trajectory_array = np.array(padded_trajectories)\n",
    "                    trajectory_mean = np.nanmean(trajectory_array, axis=0)\n",
    "                    trajectory_std = np.nanstd(trajectory_array, axis=0, ddof=1)  # CHANGE: Add ddof=1\n",
    "                    trajectory_n = np.sum(~np.isnan(trajectory_array), axis=0)  # CHANGE: Use this for SEM\n",
    "                    trajectory_sem = trajectory_std / np.sqrt(trajectory_n)  # CHANGE: Use trajectory_n\n",
    "        \n",
    "            # Store results\n",
    "            avg_parcel_traj_results[task][contrast][parcel_name] = {\n",
    "                'n_subjects': n_subjects,\n",
    "                \n",
    "                # GROUP-LEVEL\n",
    "                'slope_mean': slope_mean,\n",
    "                'slope_std': slope_std,\n",
    "                'slope_sem': slope_sem,\n",
    "                'slope_ci_lower': slope_ci_lower,\n",
    "                'slope_ci_upper': slope_ci_upper,\n",
    "                \n",
    "                'group_t_stat': group_t_stat,\n",
    "                'group_p_value': group_p_value,\n",
    "                'group_cohens_d': group_cohens_d,\n",
    "                'group_significant': group_p_value < 0.05 if not np.isnan(group_p_value) else False,\n",
    "                \n",
    "                'intercept_mean': np.mean(intercepts),\n",
    "                'intercept_std': np.std(intercepts, ddof=1),\n",
    "                'intercept_sem': np.std(intercepts, ddof=1) / np.sqrt(n_subjects),\n",
    "                \n",
    "                'r_squared_mean': np.mean(r_squareds),\n",
    "                'r_squared_std': np.std(r_squareds, ddof=1),  # ADD\n",
    "                \n",
    "                'individual_p_significant_proportion': np.mean(individual_p_values < 0.05),\n",
    "                \n",
    "                'initial_activation_mean': np.mean(initial_activations),\n",
    "                'initial_activation_std': np.std(initial_activations, ddof=1),\n",
    "                'initial_activation_sem': np.std(initial_activations, ddof=1) / np.sqrt(n_subjects),\n",
    "                \n",
    "                'final_activation_mean': np.mean(final_activations),\n",
    "                'final_activation_std': np.std(final_activations, ddof=1),\n",
    "                'final_activation_sem': np.std(final_activations, ddof=1) / np.sqrt(n_subjects),\n",
    "                \n",
    "                'percent_change_mean': np.mean(percent_changes) if len(percent_changes) > 0 else np.nan,\n",
    "                'percent_change_std': np.std(percent_changes, ddof=1) if len(percent_changes) > 1 else np.nan,\n",
    "                \n",
    "                'cohens_d_mean': np.mean(cohens_ds),\n",
    "                'cohens_d_std': np.std(cohens_ds, ddof=1),  # ADD\n",
    "                \n",
    "                'max_activation_mean': np.mean(max_activations),\n",
    "                'min_activation_mean': np.mean(min_activations),\n",
    "                'activation_range_mean': np.mean(activation_ranges),\n",
    "                \n",
    "                'trajectory_mean': trajectory_mean,\n",
    "                'trajectory_std': trajectory_std,\n",
    "                'trajectory_sem': trajectory_sem,\n",
    "                'trajectory_n_subjects': trajectory_n if 'trajectory_n' in locals() else len(trajectories),\n",
    "                \n",
    "                'positive_slope_proportion': np.mean(slopes > 0),\n",
    "                \n",
    "                'individual_slopes': slopes.tolist(),\n",
    "                'individual_intercepts': intercepts.tolist(),\n",
    "            }\n",
    "\n",
    "        n_parcels = len(avg_parcel_traj_results[task][contrast])\n",
    "        \n",
    "        if n_parcels > 0:\n",
    "            n_sig = sum(1 for p in avg_parcel_traj_results[task][contrast].values() \n",
    "                       if p['group_significant'])\n",
    "            print(f\"Completed: {n_parcels} parcels\")\n",
    "            print(f\"Group-level significant (p<0.05): {n_sig} ({100*n_sig/n_parcels:.1f}%)\")\n",
    "            count_success += 1\n",
    "        else:\n",
    "            print(f\"No parcels found (no subjects have data for this contrast): {task}|{contrast}\")\n",
    "\n",
    "print(f\"\\nAveraging complete for {count_success} task/contrasts!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e4f75e5-3006-4fd9-b1b3-8a6ba1d8001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{mean_filename}{output_ending}_averaged.pkl', 'wb') as f:\n",
    "    pickle.dump(avg_parcel_traj_results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PDM Environment)",
   "language": "python",
   "name": "pdm-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
